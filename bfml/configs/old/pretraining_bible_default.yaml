#Experiment Settings
output_directory: /rc_scratch/abeb4417/distillation_models/
log_directory: /projects/abeb4417/distillation/logs/
use_wandb: True
experiment_name: pretrained_baseline
visible_devices: "2,3"
n_gpu: 2
distillation: False
test_langs: [[eng], [tgl]]
#test_langs: [be, eu, gl, hy, kk, ml, wo, xh]

#Tokenizer Settings
tokenizer_settings:
  init:
    pretrained_model_name_or_path: xlm-roberta-base
  tokenization:
    max_length: 256
    truncation: True
    return_token_type_ids: False
    return_tensors: pt

#Data settings:
data_settings:
  data_domain: bible
  bible_settings:
    directory: /projects/abeb4417/lrl/data/bibles_dataset_clean/
    subsample: new_testament
    shuffle: False

#Training Settings
training_arguments:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 2.0e-5
  max_steps: 50000
  evaluation_strategy: 'no'
  warmup_steps: 500
  logging_steps: 250
  save_steps: 5000

#Model Settings
model_settings:
  num_hidden_layers: 6


#Evaluation Settings
use_test_set: False


