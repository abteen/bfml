#Experiment Settings
output_directory: /rc_scratch/abeb4417/distillation_models/
log_directory: /projects/abeb4417/distillation/logs/
use_wandb: False
experiment_name: pretraining_bible_kldiv
visible_devices: "1"
n_gpu: 1
#test_langs: [[eng], [tgl], [mlt]]
test_langs: [[eng], [tgl], [mlt]]
#test_langs: [be, eu, gl, hy, kk, ml, wo, xh]

distillation_settings:
  distillation_type: kldiv_identical_masking

#Tokenizer Settings
tokenizer_settings:
  init:
    pretrained_model_name_or_path: xlm-roberta-base
  tokenization:
    max_length: 256
    truncation: True
    return_token_type_ids: False
    return_tensors: pt

#Data settings:
data_settings:
  data_domain: bible
  bible_settings:
    directory: /projects/abeb4417/lrl/data/bibles_dataset_clean/
    subsample: new_testament
    shuffle: False

#Training Settings
training_arguments:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  max_steps: 50000
  evaluation_strategy: 'no'
  warmup_steps: 500
  logging_steps: 25
  save_steps: 5000

#Model Settings
model_settings:
  teacher:
    pretrained_model_name_or_path: xlm-roberta-base
  student:
    pretrained_model_name_or_path: xlm-roberta-base
    num_hidden_layers: 6


#Evaluation Settings
use_test_set: False


