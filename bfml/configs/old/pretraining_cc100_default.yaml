#Experiment Settings
output_directory: /rc_scratch/abeb4417/distillation_models/
log_directory: /projects/abeb4417/distillation/logs/
use_wandb: False
experiment_name: distillation_test
visible_devices: "0,1"
n_gpu: 2
distillation: False
test_langs: [[be]]
#test_langs: [be, eu, gl, hy, kk, ml, wo, xh]

#Tokenizer Settings
tokenizer_settings:
  init:
    pretrained_model_name_or_path: xlm-roberta-base
  tokenization:
    max_length: 256
    truncation: True
    return_token_type_ids: False
    return_tensors: pt

#Data settings:
data_settings:
  data_domain: cc100
  load_dataset_settings:
    split: train
  downsample: True
  downsample_final_size: 250000
  downsample_settings:
    shuffle: True
    seed: 42

#Training Settings
training_arguments:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  max_steps: 50000
  evaluation_strategy: 'no'
  warmup_steps: 5
  logging_steps: 500
  save_steps: 10000

#Model Settings
model_settings:
  num_hidden_layers: 6


#Evaluation Settings
use_test_set: False


