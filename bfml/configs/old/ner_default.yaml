#Experiment Settings

output_directory: /rc_scratch/abeb4417/distillation_models/
log_directory: /projects/abeb4417/distillation/logs/ner/finetune
use_wandb: False
experiment_name: finetune_ner_kldiv_all_tokens
visible_devices: "0"
n_gpu: 1
eval_only: True

distillation_settings:
    distillation_type: none


#Task Settings
task: ner
task_settings:
    data_directory: /projects/abeb4417/lrl/data/ner/rahimi_output/
    train_language: same
    eval_language: same
    test_language: eng
    max_len: 256

#Tokenizer Settings
tokenizer_settings:
  init:
    pretrained_model_name_or_path: xlm-roberta-base
  tokenization:
    max_length: 256
    truncation: True
    return_token_type_ids: False
    return_tensors: pt


training_arguments:
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 2.0e-5
  num_train_epochs: 5
  evaluation_strategy: 'epoch'
  logging_steps: 25
  save_steps: 40000

model_settings:
  init:
    num_labels: 9
