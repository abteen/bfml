import torch, numpy as np, os
from datasets import load_dataset, concatenate_datasets


def read_lines(file, preserve_lines=True):
    """

    :param file: File to read from line-by-line
    :param preserve_lines: If True, return a list n items, each corresponding to a line.
                        Else, return a list of 1 item consisting of all lines joined together
    :return: List[str]
    """

    file_lines = []
    with open(file, 'r') as f:
        for line in f:
            file_lines.append(line.strip())

    if not preserve_lines:
        collapsed_document = ' '.join(line for line in file_lines)
        return [collapsed_document]

    return file_lines

def chunk_and_tokenize_documents(documents, tokenizer, max_length):
    """

    :param documents: List[String] where each element is a document
    :param tokenizer: tokenizer
    :param max_length: length of sequences which will be inputted into the model
    :return: input_ids and attention masks generated by chunking each document into tensors of size max_length.
                No overlap between tensors. Does not respect sentence boundaries, but respects document boundaries.
    """

    bos_token = torch.tensor([tokenizer.bos_token_id], dtype=torch.int64)
    eos_token = torch.tensor([tokenizer.eos_token_id], dtype=torch.int64)
    pad_token = torch.tensor([tokenizer.pad_token_id], dtype=torch.int64)
    zero_token = torch.tensor([0], dtype=torch.int64) #In case pad token != 0
    double_1 = torch.tensor([1,1], dtype=torch.int64)

    max_len = max_length - 2

    final_input_ids = []
    attention_masks = []

    for document in documents:

        a = tokenizer(document, return_tensors='pt', add_special_tokens=False)

        input_ids = torch.split(a['input_ids'][0], max_len)
        attention_mask = torch.split(a['attention_mask'][0], max_len)

        assert len(input_ids) == len(attention_mask)

        for i in range(len(input_ids)):

            inp = torch.cat([bos_token, input_ids[i], eos_token])
            attn = torch.cat((attention_mask[i], double_1))

            if len(inp) < max_length:
                pad_len = max_length-inp.shape[0]
                inp = torch.cat([inp, pad_token.expand(pad_len)])
                attn = torch.cat([attn, zero_token.expand(pad_len)])

            final_input_ids.append(inp)
            attention_masks.append(attn)

    stacked_input_ids = torch.stack(final_input_ids)
    stacked_attentions = torch.stack(attention_masks)


    return stacked_input_ids, stacked_attentions


def chunk_and_tokenize_documents_for_dataset_torch(documents, tokenizer, max_length):
    """
        Torch implementation
    :param documents: List[String] where each element is a document
    :param tokenizer: tokenizer
    :param max_length: length of sequences which will be inputted into the model
    :return: input_ids and attention masks generated by chunking each document into tensors of size max_length.
                No overlap between tensors. Does not respect sentence boundaries, but respects document boundaries.
    """

    bos_token = torch.tensor([tokenizer.bos_token_id], dtype=torch.int64)
    eos_token = torch.tensor([tokenizer.eos_token_id], dtype=torch.int64)
    pad_token = torch.tensor([tokenizer.pad_token_id], dtype=torch.int64)
    zero_token = torch.tensor([0], dtype=torch.int64) #In case pad token != 0
    double_1 = torch.tensor([1,1], dtype=torch.int64)

    max_len = max_length - 2

    final_input_ids = []
    attention_masks = []

    for document in documents:

        a = tokenizer(document, return_tensors='pt', add_special_tokens=False)

        input_ids = torch.split(a['input_ids'][0], max_len)
        attention_mask = torch.split(a['attention_mask'][0], max_len)

        assert len(input_ids) == len(attention_mask)

        for i in range(len(input_ids)):

            inp = torch.cat([bos_token, input_ids[i], eos_token])
            attn = torch.cat((attention_mask[i], double_1))

            if len(inp) < max_length:
                pad_len = max_length-inp.shape[0]
                inp = torch.cat([inp, pad_token.expand(pad_len)])
                attn = torch.cat([attn, zero_token.expand(pad_len)])

            final_input_ids.append(inp)
            attention_masks.append(attn)

    return {
        'input_ids': final_input_ids,
        'attention_mask': attention_masks
    }


def chunk_and_tokenize_documents_for_dataset_np(documents, tokenizer, max_length):
    """
        Numpy implementation
    :param documents: List[String] where each element is a document
    :param tokenizer: tokenizer
    :param max_length: length of sequences which will be inputted into the model
    :return: input_ids and attention masks generated by chunking each document into tensors of size max_length.
                No overlap between tensors. Does not respect sentence boundaries, but respects document boundaries.
    """


    max_len = max_length - 2

    final_input_ids = []
    attention_masks = []

    for i, document in enumerate(documents):

        a = tokenizer(document, return_tensors='np', add_special_tokens=False)
        input_ids = a['input_ids'] #shape = (1, sequence_length)


        end_position = input_ids.shape[1] % max_len
        pad_num = (max_len - end_position) % max_len #stay in mod space to prevent all 0s when perfectly divisible

        input_ids = np.pad(input_ids, [(0,0), (0, pad_num)], mode='constant', constant_values=0)
        input_ids = np.reshape(input_ids, (-1, max_len))
        input_ids = np.pad(input_ids, [(0,0), (1,1)], mode='constant', constant_values=(1,2))

        if pad_num > 0:
            input_ids[-1][end_position+1] = 2
            input_ids[-1][-1] = 0


        attention_mask = (input_ids != 0).astype('int64')


        final_input_ids.extend(np.split(input_ids, input_ids.shape[0]))
        attention_masks.extend(np.split(attention_mask, attention_mask.shape[0]))


    return {
        'input_ids': final_input_ids,
        'attention_mask': attention_masks
    }


def chunk_and_tokenize_documents_python(documents, tokenizer, max_length):
    """
        Python Implementation, parts taken from HF datasets.

    :param documents: List[String] where each element is a document
    :param tokenizer: tokenizer
    :param max_length: length of sequences which will be inputted into the model
    :return: input_ids and attention masks generated by chunking each document into tensors of size max_length.
                No overlap between tensors. Does not respect sentence boundaries, but respects document boundaries.
    """


    max_len = max_length - 2


    final_input_ids = []
    attention_masks = []

    for document in documents:

        a = tokenizer(document, add_special_tokens=False)

        i = 0 # initialize for safety in case seq length < max_len
        for i in range(0, len(a['input_ids']) - max_len + 1, max_len):
            final_input_ids.append(tokenizer.build_inputs_with_special_tokens(a['input_ids'][i:i+max_len]))
            attention_masks.append([1] * max_length)


        i = i+max_len if i != 0 else 0 #iterate to check for leftovers

        if i < len(a['input_ids']):
            leftover_tokens = tokenizer.build_inputs_with_special_tokens(a['input_ids'][i:len(a['input_ids'])])
            attention_mask = [1] * len(leftover_tokens)

            pad_val = max_length - len(leftover_tokens)
            final_input_ids.append(leftover_tokens + [tokenizer.pad_token_id] * pad_val)
            attention_masks.append(attention_mask + [0] * pad_val)



    return {
        'input_ids': final_input_ids,
        'attention_mask': attention_masks
    }

def load_data_from_dir(input_dir, tokenizer, max_length=256):
    """
    Load data directly from text files, tokenize, and chunk.
    :param input_directory:
    :param tokenizer:
    :return:
    """

    num_processes = int(os.environ['SLURM_NTASKS'])

    print('{} processes Available'.format(num_processes))

    if os.path.isdir(input_dir):
        data_files = sorted([os.path.join(input_dir, x) for x in os.listdir(input_dir)])
    else:
        data_files = [input_dir]

    created_datasets = []
    for i, data_file in enumerate(data_files):
        data = load_dataset('text', data_files=data_file)
        data = data.filter(lambda ex: len(ex['text']) >= 50)

        data = data.map(
            lambda examples: chunk_and_tokenize_documents_python(examples['text'], tokenizer, max_length), batched=True,
            remove_columns=['text'],
            num_proc=num_processes)

        created_datasets.append(data)
        print('.................Finished with dataset {} of {}'.format(i, len(data_files)))

    created_datasets = [x['train'] for x in created_datasets]
    print('concating:')
    final_dataset = concatenate_datasets(created_datasets)
    return final_dataset