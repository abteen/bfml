import torch
from tqdm import tqdm


def read_lines(file, preserve_lines=True):
    """

    :param file: File to read from line-by-line
    :param preserve_lines: If True, return a list n items, each corresponding to a line.
                        Else, return a list of 1 item consisting of all lines joined together
    :return: List[str]
    """

    file_lines = []
    with open(file, 'r') as f:
        for line in f:
            file_lines.append(line.strip())

    if not preserve_lines:
        collapsed_document = ' '.join(line for line in file_lines)
        return [collapsed_document]

    return file_lines

# @memory.cache
def chunk_and_tokenize_documents(documents, tokenizer, max_length):
    """

    :param documents: List[String] where each element is a document
    :param tokenizer: tokenizer
    :param max_length: length of sequences which will be inputted into the model
    :return: input_ids and attention masks generated by chunking each document into tensors of size max_length.
                No overlap between tensors. Does not respect sentence boundaries, but respects document boundaries.
    """

    bos_token = torch.tensor([tokenizer.bos_token_id], dtype=torch.int64)
    eos_token = torch.tensor([tokenizer.eos_token_id], dtype=torch.int64)
    pad_token = torch.tensor([tokenizer.pad_token_id], dtype=torch.int64)
    zero_token = torch.tensor([0], dtype=torch.int64) #In case pad token != 0
    double_1 = torch.tensor([1,1], dtype=torch.int64)

    max_len = max_length - 2

    final_input_ids = []
    attention_masks = []

    for document in tqdm(documents):

        a = tokenizer(document, return_tensors='pt', add_special_tokens=False)

        input_ids = torch.split(a['input_ids'][0], max_len)
        attention_mask = torch.split(a['attention_mask'][0], max_len)

        assert len(input_ids) == len(attention_mask)

        for i in range(len(input_ids)):

            inp = torch.cat([bos_token, input_ids[i], eos_token])
            attn = torch.cat((attention_mask[i], double_1))

            if len(inp) < max_length:
                pad_len = max_length-inp.shape[0]
                inp = torch.cat([inp, pad_token.expand(pad_len)])
                attn = torch.cat([attn, zero_token.expand(pad_len)])

            final_input_ids.append(inp)
            attention_masks.append(attn)

    stacked_input_ids = torch.stack(final_input_ids)
    stacked_attentions = torch.stack(attention_masks)


    return stacked_input_ids, stacked_attentions



